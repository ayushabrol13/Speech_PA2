{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speech Understanding | Programming Assignment 2\n",
    "\n",
    "    Ayush Abrol B20AI052\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import os\n",
    "import wandb\n",
    "import random\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Union\n",
    "\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import confusion_matrix\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.optimize import brentq\n",
    "import fire\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.functional import pad\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "from torchvision import transforms, utils, datasets, models\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch import Tensor\n",
    "from torchaudio._internal import download_url_to_file\n",
    "from torchaudio.transforms import Resample\n",
    "import torchaudio.transforms as trans\n",
    "from torchaudio.datasets.utils import _extract_zip, _load_waveform\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: In speaker verification, the training dataset consists of audio clips paired with speaker IDs, denoted as (D = (xi, yi)). Given an audio clip (x) and a reference clip (x0), the objective is to ascertain whether (x0) and (x) belong to the same speaker. \n",
    "\n",
    "**Tasks**: \n",
    "\n",
    "    1. Choose three pre-trained models from the list: ’ecapa tdnn’, ’hubert large’, ’wav2vec2 xlsr’, ’unispeech sat’, ’wavlm base plus’, ’wavlm large’ trained on the VoxCeleb1 dataset. You can find the pre-trained models on this link.\n",
    "    \n",
    "    2. —- Calculate the EER(%) on the VoxCeleb1-H dataset using the above selected models. You can get the \n",
    "    dataset from here.\n",
    "    \n",
    "    3. Compare your result with Table II of the WavLM paper. \n",
    "\n",
    "    4. Evaluate the selected models on the test set of any one Indian language of the Kathbath Dataset. Report the EER(%). \n",
    "    \n",
    "    5. Fine-tune, the best model on the validation set of the selected language of Kathbath Dataset. Report the EER(%). \n",
    "    \n",
    "    6. Provide an analysis of the results along with plausible reasons for the observed outcomes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_eer_percentage(labels, preds):\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(labels, preds, pos_label=1)\n",
    "    eer = brentq(lambda x: 1. - x - interp1d(fpr, tpr)(x), 0., 1.)\n",
    "    return eer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Res2Conv1dReluBn(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True, scale=4):\n",
    "        super().__init__()\n",
    "        assert channels % scale == 0, \"{} % {} != 0\".format(channels, scale)\n",
    "        self.scale = scale\n",
    "        self.width = channels // scale\n",
    "        self.nums = scale if scale == 1 else scale - 1\n",
    "\n",
    "        self.convs = []\n",
    "        self.bns = []\n",
    "        for i in range(self.nums):\n",
    "            self.convs.append(nn.Conv1d(self.width, self.width, kernel_size, stride, padding, dilation, bias=bias))\n",
    "            self.bns.append(nn.BatchNorm1d(self.width))\n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        self.bns = nn.ModuleList(self.bns)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = []\n",
    "        spx = torch.split(x, self.width, 1)\n",
    "        for i in range(self.nums):\n",
    "            if i == 0:\n",
    "                sp = spx[i]\n",
    "            else:\n",
    "                sp = sp + spx[i]\n",
    "            # Order: conv -> relu -> bn\n",
    "            sp = self.convs[i](sp)\n",
    "            sp = self.bns[i](F.relu(sp))\n",
    "            out.append(sp)\n",
    "        if self.scale != 1:\n",
    "            out.append(spx[self.nums])\n",
    "        out = torch.cat(out, dim=1)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Conv1dReluBn(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=1, stride=1, padding=0, dilation=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, padding, dilation, bias=bias)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.bn(F.relu(self.conv(x)))\n",
    "\n",
    "\n",
    "class SE_Connect(nn.Module):\n",
    "    def __init__(self, channels, se_bottleneck_dim=128):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(channels, se_bottleneck_dim)\n",
    "        self.linear2 = nn.Linear(se_bottleneck_dim, channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = x.mean(dim=2)\n",
    "        out = F.relu(self.linear1(out))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        out = x * out.unsqueeze(2)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SE_Res2Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, scale, se_bottleneck_dim):\n",
    "        super().__init__()\n",
    "        self.Conv1dReluBn1 = Conv1dReluBn(in_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.Res2Conv1dReluBn = Res2Conv1dReluBn(out_channels, kernel_size, stride, padding, dilation, scale=scale)\n",
    "        self.Conv1dReluBn2 = Conv1dReluBn(out_channels, out_channels, kernel_size=1, stride=1, padding=0)\n",
    "        self.SE_Connect = SE_Connect(out_channels, se_bottleneck_dim)\n",
    "\n",
    "        self.shortcut = None\n",
    "        if in_channels != out_channels:\n",
    "            self.shortcut = nn.Conv1d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        if self.shortcut:\n",
    "            residual = self.shortcut(x)\n",
    "\n",
    "        x = self.Conv1dReluBn1(x)\n",
    "        x = self.Res2Conv1dReluBn(x)\n",
    "        x = self.Conv1dReluBn2(x)\n",
    "        x = self.SE_Connect(x)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "class AttentiveStatsPool(nn.Module):\n",
    "    def __init__(self, in_dim, attention_channels=128, global_context_att=False):\n",
    "        super().__init__()\n",
    "        self.global_context_att = global_context_att\n",
    "\n",
    "        # Use Conv1d with stride == 1 rather than Linear, then we don't need to transpose inputs.\n",
    "        if global_context_att:\n",
    "            self.linear1 = nn.Conv1d(in_dim * 3, attention_channels, kernel_size=1)  # equals W and b in the paper\n",
    "        else:\n",
    "            self.linear1 = nn.Conv1d(in_dim, attention_channels, kernel_size=1)  # equals W and b in the paper\n",
    "        self.linear2 = nn.Conv1d(attention_channels, in_dim, kernel_size=1)  # equals V and k in the paper\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        if self.global_context_att:\n",
    "            context_mean = torch.mean(x, dim=-1, keepdim=True).expand_as(x)\n",
    "            context_std = torch.sqrt(torch.var(x, dim=-1, keepdim=True) + 1e-10).expand_as(x)\n",
    "            x_in = torch.cat((x, context_mean, context_std), dim=1)\n",
    "        else:\n",
    "            x_in = x\n",
    "\n",
    "        # DON'T use ReLU here! In experiments, I find ReLU hard to converge.\n",
    "        alpha = torch.tanh(self.linear1(x_in))\n",
    "        # alpha = F.relu(self.linear1(x_in))\n",
    "        alpha = torch.softmax(self.linear2(alpha), dim=2)\n",
    "        mean = torch.sum(alpha * x, dim=2)\n",
    "        residuals = torch.sum(alpha * (x ** 2), dim=2) - mean ** 2\n",
    "        std = torch.sqrt(residuals.clamp(min=1e-9))\n",
    "        return torch.cat([mean, std], dim=1)\n",
    "\n",
    "\n",
    "class ECAPA_TDNN(nn.Module):\n",
    "    def __init__(self, feat_dim=80, channels=512, emb_dim=192, global_context_att=False,\n",
    "                 feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n",
    "        super().__init__()\n",
    "\n",
    "        self.feat_type = feat_type\n",
    "        self.feature_selection = feature_selection\n",
    "        self.update_extract = update_extract\n",
    "        self.sr = sr\n",
    "\n",
    "        if feat_type == \"fbank\" or feat_type == \"mfcc\":\n",
    "            self.update_extract = False\n",
    "\n",
    "        win_len = int(sr * 0.025)\n",
    "        hop_len = int(sr * 0.01)\n",
    "\n",
    "        if feat_type == 'fbank':\n",
    "            self.feature_extract = trans.MelSpectrogram(sample_rate=sr, n_fft=512, win_length=win_len,\n",
    "                                                        hop_length=hop_len, f_min=0.0, f_max=sr // 2,\n",
    "                                                        pad=0, n_mels=feat_dim)\n",
    "        elif feat_type == 'mfcc':\n",
    "            melkwargs = {\n",
    "                'n_fft': 512,\n",
    "                'win_length': win_len,\n",
    "                'hop_length': hop_len,\n",
    "                'f_min': 0.0,\n",
    "                'f_max': sr // 2,\n",
    "                'pad': 0\n",
    "            }\n",
    "            self.feature_extract = trans.MFCC(sample_rate=sr, n_mfcc=feat_dim, log_mels=False,\n",
    "                                              melkwargs=melkwargs)\n",
    "        else:\n",
    "            \n",
    "            self.feature_extract = torch.hub.load('s3prl/s3prl', feat_type)\n",
    "            \n",
    "            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[23].self_attn, \"fp32_attention\"):\n",
    "                self.feature_extract.model.encoder.layers[23].self_attn.fp32_attention = False\n",
    "            if len(self.feature_extract.model.encoder.layers) == 24 and hasattr(self.feature_extract.model.encoder.layers[11].self_attn, \"fp32_attention\"):\n",
    "                self.feature_extract.model.encoder.layers[11].self_attn.fp32_attention = False\n",
    "\n",
    "            self.feat_num = self.get_feat_num()\n",
    "            self.feature_weight = nn.Parameter(torch.zeros(self.feat_num))\n",
    "\n",
    "        if feat_type != 'fbank' and feat_type != 'mfcc':\n",
    "            freeze_list = ['final_proj', 'label_embs_concat', 'mask_emb', 'project_q', 'quantizer']\n",
    "            for name, param in self.feature_extract.named_parameters():\n",
    "                for freeze_val in freeze_list:\n",
    "                    if freeze_val in name:\n",
    "                        param.requires_grad = False\n",
    "                        break\n",
    "\n",
    "        if not self.update_extract:\n",
    "            for param in self.feature_extract.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.instance_norm = nn.InstanceNorm1d(feat_dim)\n",
    "        # self.channels = [channels] * 4 + [channels * 3]\n",
    "        self.channels = [channels] * 4 + [1536]\n",
    "\n",
    "        self.layer1 = Conv1dReluBn(feat_dim, self.channels[0], kernel_size=5, padding=2)\n",
    "        self.layer2 = SE_Res2Block(self.channels[0], self.channels[1], kernel_size=3, stride=1, padding=2, dilation=2, scale=8, se_bottleneck_dim=128)\n",
    "        self.layer3 = SE_Res2Block(self.channels[1], self.channels[2], kernel_size=3, stride=1, padding=3, dilation=3, scale=8, se_bottleneck_dim=128)\n",
    "        self.layer4 = SE_Res2Block(self.channels[2], self.channels[3], kernel_size=3, stride=1, padding=4, dilation=4, scale=8, se_bottleneck_dim=128)\n",
    "\n",
    "        # self.conv = nn.Conv1d(self.channels[-1], self.channels[-1], kernel_size=1)\n",
    "        cat_channels = channels * 3\n",
    "        self.conv = nn.Conv1d(cat_channels, self.channels[-1], kernel_size=1)\n",
    "        self.pooling = AttentiveStatsPool(self.channels[-1], attention_channels=128, global_context_att=global_context_att)\n",
    "        self.bn = nn.BatchNorm1d(self.channels[-1] * 2)\n",
    "        self.linear = nn.Linear(self.channels[-1] * 2, emb_dim)\n",
    "\n",
    "\n",
    "    def get_feat_num(self):\n",
    "        self.feature_extract.eval()\n",
    "        wav = [torch.randn(self.sr).to(next(self.feature_extract.parameters()).device)]\n",
    "        with torch.no_grad():\n",
    "            features = self.feature_extract(wav)\n",
    "        select_feature = features[self.feature_selection]\n",
    "        if isinstance(select_feature, (list, tuple)):\n",
    "            return len(select_feature)\n",
    "        else:\n",
    "            return 1\n",
    "\n",
    "    def get_feat(self, x):\n",
    "        if self.update_extract:\n",
    "            x = self.feature_extract([sample for sample in x])\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                if self.feat_type == 'fbank' or self.feat_type == 'mfcc':\n",
    "                    x = self.feature_extract(x) + 1e-6  # B x feat_dim x time_len\n",
    "                else:\n",
    "                    x = self.feature_extract([sample for sample in x])\n",
    "\n",
    "        if self.feat_type == 'fbank':\n",
    "            x = x.log()\n",
    "\n",
    "        if self.feat_type != \"fbank\" and self.feat_type != \"mfcc\":\n",
    "            x = x[self.feature_selection]\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = torch.stack(x, dim=0)\n",
    "            else:\n",
    "                x = x.unsqueeze(0)\n",
    "            norm_weights = F.softmax(self.feature_weight, dim=-1).unsqueeze(-1).unsqueeze(-1).unsqueeze(-1)\n",
    "            x = (norm_weights * x).sum(dim=0)\n",
    "            x = torch.transpose(x, 1, 2) + 1e-6\n",
    "\n",
    "        x = self.instance_norm(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.get_feat(x)\n",
    "\n",
    "        out1 = self.layer1(x)\n",
    "        out2 = self.layer2(out1)\n",
    "        out3 = self.layer3(out2)\n",
    "        out4 = self.layer4(out3)\n",
    "\n",
    "        out = torch.cat([out2, out3, out4], dim=1)\n",
    "        out = F.relu(self.conv(out))\n",
    "        out = self.bn(self.pooling(out))\n",
    "        out = self.linear(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "def ECAPA_TDNN_SMALL(feat_dim, emb_dim=256, feat_type='fbank', sr=16000, feature_selection=\"hidden_states\", update_extract=False, config_path=None):\n",
    "    return ECAPA_TDNN(feat_dim=feat_dim, channels=512, emb_dim=emb_dim,\n",
    "                      feat_type=feat_type, sr=sr, feature_selection=feature_selection, update_extract=update_extract, config_path=config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(model_name, checkpoint=None):\n",
    "    if model_name == 'unispeech_sat':\n",
    "        config_path = 'models/unispeech_sat.th'\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='unispeech_sat', config_path=config_path)\n",
    "    elif model_name == 'wavlm_base_plus':\n",
    "        config_path = None\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=768, feat_type='wavlm_base_plus', config_path=config_path)\n",
    "    elif model_name == 'wavlm_large':\n",
    "        config_path = None\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='wavlm_large', config_path=config_path)\n",
    "    elif model_name == 'hubert_large':\n",
    "        config_path = None\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='hubert_large_ll60k', config_path=config_path)\n",
    "    elif model_name == 'wav2vec2_xlsr':\n",
    "        config_path = None\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=1024, feat_type='wav2vec2_xlsr', config_path=config_path)\n",
    "    else:\n",
    "        model = ECAPA_TDNN_SMALL(feat_dim=40, feat_type='fbank')\n",
    "\n",
    "    if checkpoint is not None:\n",
    "        state_dict = torch.load(checkpoint, map_location=lambda storage, loc: storage)\n",
    "        model.load_state_dict(state_dict['model'], strict=False)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verification(model,  wav1, wav2, use_gpu=True, checkpoint=None):\n",
    "    resample1 = Resample(orig_freq=16000, new_freq=16000)\n",
    "    resample2 = Resample(orig_freq=16000, new_freq=16000)\n",
    "    wav1 = resample1(wav1)\n",
    "    wav2 = resample2(wav2)\n",
    "\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "        wav1 = wav1.cuda()\n",
    "        wav2 = wav2.cuda()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        emb1 = model(wav1)\n",
    "        emb2 = model(wav2)\n",
    "\n",
    "    sim = F.cosine_similarity(emb1, emb2)\n",
    "    print(\"The similarity score between two audios is {:.4f} (-1.0, 1.0).\".format(sim[0].item()))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VoXCeleb1 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetVox(Dataset):\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        self.src1, self.src2, self.labels = self.read_data()\n",
    "\n",
    "    def read_data(self):\n",
    "        src1 = []\n",
    "        src2 = []\n",
    "        labels = []\n",
    "\n",
    "        datafile = open(self.root, 'r')\n",
    "        for dataline in datafile:\n",
    "            dataline = dataline.strip().split()\n",
    "            src1.append(dataline[1])\n",
    "            src2.append(dataline[2])\n",
    "            labels.append(dataline[0])\n",
    "        \n",
    "        return src1, src2, labels\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src1, src1_samplingrate = torchaudio.load(os.path.join(\"/root/voxceleb1/wav/\", self.src1[idx]))\n",
    "        src2, src2_samplingrate = torchaudio.load(os.path.join(\"/root/voxceleb1/wav/\", self.src2[idx]))\n",
    "\n",
    "        if src1.size(1) < 16000:\n",
    "            src1 = pad(src1, (0, 16000 - src1.size(1)))\n",
    "        else:\n",
    "            src1 = src1[:, :16000]\n",
    "        \n",
    "        if src2.size(1) < 16000:\n",
    "            src2 = pad(src2, (0, 16000 - src2.size(1)))\n",
    "        else:\n",
    "            src2 = src2[:, :16000]\n",
    "        \n",
    "        return src1[0], src2[0], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dataloader\n",
    "dataset = AudioDatasetVox(root=\"root/VoxCelebVerification.txt\")\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise model\n",
    "model = init_model(\"wavlm_base_plus\", checkpoint=\"checkpoints/wavlm_base_plus.pt\")\n",
    "model.eval()\n",
    "\n",
    "#calculate eer for each model\n",
    "eer_batchwise = []\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    src1, src2, label = data[0], data[1], data[2]\n",
    "\n",
    "    src1 = src1\n",
    "    src2 = src2\n",
    "\n",
    "    similarity_scores = verification(model, src1, src2)\n",
    "    print(similarity_scores)\n",
    "    similarity_scores = (similarity_scores >= 0.0).float()\n",
    "    predictions = similarity_scores\n",
    "\n",
    "    eer_score = calculate_eer_percentage(label, predictions)\n",
    "    print(\"[Batch {}] EER score: {}\".format(i, eer_score))\n",
    "    eer_batchwise.append(eer_score)\n",
    "\n",
    "print(\"EER Score:\", sum(eer_batchwise)/len(eer_batchwise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioDatasetKathBath(Dataset):\n",
    "    def __init__(self, root):\n",
    "\n",
    "        self.root = root\n",
    "        self.src1, self.src2, self.labels = self.read_data()\n",
    "        self.read_hindidata()\n",
    "\n",
    "    def read_data(self):\n",
    "        src1 = []\n",
    "        src2 = []\n",
    "        labels = []\n",
    "\n",
    "        file = open(self.root, 'r')\n",
    "        for line in file:\n",
    "            line = line.strip().split()\n",
    "            src1.append(line[1])\n",
    "            src2.append(line[2])\n",
    "            labels.append(int(line[0]))\n",
    "        \n",
    "        return src1, src2, labels\n",
    "    \n",
    "    def read_hindidata(self):\n",
    "\n",
    "        for i in range(len(self.src1)):\n",
    "            \n",
    "            split_path = self.src1[i].split('hindi')[1]\n",
    "            split_path = split_path.split('/')\n",
    "            \n",
    "            new_path = os.path.join(\"test_known\",split_path[2],split_path[4])\n",
    "            new_path = os.path.join(\"root/KathBath\",new_path)\n",
    "            \n",
    "            new_path = new_path.replace('.wav', '.m4a')\n",
    "            \n",
    "            self.src1[i] = new_path\n",
    "\n",
    "        for i in range(len(self.src2)):\n",
    "            \n",
    "            split_path = self.src2[i].split('hindi')[1]\n",
    "            split_path = split_path.split('/')\n",
    "            \n",
    "            new_path = os.path.join(\"test_known\",split_path[2],split_path[4])\n",
    "            new_path = os.path.join(\"root/KathBath\",new_path)\n",
    "            \n",
    "            new_path = new_path.replace('.wav', '.m4a')\n",
    "            \n",
    "            self.src2[i] = new_path\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src1, src1_samplingrate = librosa.load(self.src1[idx])\n",
    "        src2, src2_samplingrate = librosa.load(self.src2[idx])\n",
    "\n",
    "        src1 = torch.tensor(src1, dtype=torch.float32)\n",
    "        src1 = src1.unsqueeze(0)\n",
    "        src2 = torch.tensor(src2, dtype=torch.float32)\n",
    "        src2 = src2.unsqueeze(0)\n",
    "\n",
    "        if src1.size(1) < 16000:\n",
    "            src1 = pad(src1, (0, 16000 - src1.size(1)))\n",
    "        else:\n",
    "            src1 = src1[:, :16000]\n",
    "        \n",
    "        if src2.size(1) < 16000:\n",
    "            src2 = pad(src2, (0, 16000 - src2.size(1)))\n",
    "        else:\n",
    "            src2 = src2[:, :16000]\n",
    "        \n",
    "        return src1[0], src2[0], self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise dataloader\n",
    "dataset = AudioDatasetKathBath(root=\"root/KathBath_Metadata/hindi/test_known_data.txt\")\n",
    "dataloader = DataLoader(dataset, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise model\n",
    "model = init_model(\"wavlm_base_plus\", checkpoint=\"checkpoints/WavLM-Base+.pt\")\n",
    "model.eval()\n",
    "\n",
    "#calculate eer values\n",
    "eer_batchwise = []\n",
    "\n",
    "for i, data in enumerate(dataloader):\n",
    "\n",
    "    src1, src2, label = data[0], data[1], data[2]\n",
    "\n",
    "    src1 = src1\n",
    "    src2 = src2\n",
    "\n",
    "    similarity_scores = verification(model, src1, src2)\n",
    "    print(similarity_scores)\n",
    "    similarity_scores = (similarity_scores >= 0.0).float()\n",
    "    predictions = similarity_scores\n",
    "\n",
    "    eer_score = calculate_eer_percentage(label, predictions)\n",
    "    print(\"[Batch {}] EER score: {}\".format(i, eer_score))\n",
    "    eer_batchwise.append(eer_score)\n",
    "\n",
    "print(\"EER Score:\", sum(eer_batchwise)/len(eer_batchwise))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradio for Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "def verify_audio_model1(model, audio1, audio2):\n",
    "    # Verify similarity between two audio files using model 1\n",
    "    audio1_data = audio1[1] / np.max(np.abs(audio1[1]))\n",
    "    audio2_data = audio2[1] / np.max(np.abs(audio2[1]))\n",
    "    audio1_tensor = torch.tensor(audio1_data, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    audio2_tensor = torch.tensor(audio2_data, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "    similarity = verification(model, audio1_tensor, audio2_tensor)\n",
    "    return str(similarity.cpu().detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialise model\n",
    "model = init_model(\"wavlm_base_plus\", checkpoint=\"checkpoints/wavelmbase+/WavLM-Base+.pt\").to(device)\n",
    "model.eval()\n",
    "\n",
    "# Gradio interface for audio verification using model 1\n",
    "audio1_input = gr.components.Audio(source=\"upload\", label=\"Upload test sample 1\", type=\"numpy\")\n",
    "audio2_input = gr.components.Audio(source=\"upload\", label=\"Upload test sample 2\", type=\"numpy\")\n",
    "similarity_output = gr.outputs.Label(label=\"s=Simimlarity Score\")\n",
    "gr.Interface(fn=verify_audio_model1, inputs=[audio1_input, audio2_input], outputs=similarity_output).launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal**: The goal of speech separation is to estimate individual speaker signals from their mixture, where the source signals may be overlapped with each other entirely or partially.\n",
    "\n",
    "**Tasks**: \n",
    "\n",
    "    1. Generate the LibriMix dataset by combining two speakers from the LibriSpeech dataset, focusing solely on the LibriSpeech test clean partition. \n",
    "\n",
    "    2. Partition the resulting LibriMix dataset into a 70-30 split for training and testing purposes. Evaluate the performance of the pre-trained SepFormer on the testing set, employing scale-invariant signal-to-noise ratio improvement (SISNRi) and signal-to-distortion ratio improvement (SDRi) as metrics. \n",
    "\n",
    "    3. Fine-tune the SepFormer model using the training set and report its performance on the test split of the LibriMix dataset.\n",
    "\n",
    "    4. Provide observations on the changes in performance throughout the experiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: Q2 Done in .py files"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
